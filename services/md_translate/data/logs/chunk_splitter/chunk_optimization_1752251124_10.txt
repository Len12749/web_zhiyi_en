===== 文本块对 10 =====

===== LLM PROMPT =====

您是一位专精于文档处理的AI助手。我需要您优化两个连续文本块之间的截断点，使得每个块在语义上更加完整，便于后续单独处理（如翻译）时保持正确的上下文。

您的任务是:
1. 分析这两个文本块的语义内容
2. 确定最佳的语义断点位置
3. 重新分配文本内容，使每个块在语义上更完整
4. 确保输出格式使用markdown内联latex格式，保持数学公式的正确性

输入的两个文本块:
{
"text_a": "where $g_{9}(c,\\vartheta,r,q,\\tau,k)=\\vartheta_{g}(c,\\vartheta,r,q,\\tau,k)/\\partial\\vartheta$ ; with $g$ defined by Eq. (C.1), and where $c_{i}=f(V_{i}^{A},\\vartheta_{0},r_{i}^{A},q_{i}^{A},\\tau_{i},k_{i})$ : The first term on the right-hand side of Eq. (C.7) arises from the explicit dependence of $h$ on W; while the second term arises from the dependence of $h$ on $V_{i}$ and the dependence of $V_{i}^{9}=g(c_{i},\\vartheta,r_{i}^{A},q_{i}^{A},\\tau_{i},k_{i})$ on W; for $i\\in\\{n-n_{v}+1,...,n\\}$ : This second term is important in identifying risk-premium parameters such as $\\eta^{v}$ : Intuitively, such parameters are identified by exploring the option-pricing relation through $V^{\\vartheta}$ :\n\nAssumption C.6 (Convergence of \u2018\u2018Jacobian estimator\u2019\u2019). For some constant $(n_{h}\\times n_{\\vartheta})$ matrix $d_{0}$ of rank $n_{\\vartheta}$ : (i) $1/N\\textstyle\\sum_{n\\leqslant N}d(\\vartheta_{0},X_{n},Y_{n})$ converges in probability as $N\\to\\infty$ to $d_{0}$ : (ii) For any $\\{\\mathcal{\\vartheta}_{n}\\}$ converging in probability as $n\\to\\infty$ to $\\vartheta_{0}$ ; $1/N\\textstyle\\sum_{n\\leqslant N}d(\\vartheta_{n},X_{n},Y_{n})$ converges in probability as $N\\to\\infty$ to $d_{0}$ :\n\n<!-- Page:39 -->\n\nPart (i) of Assumption C.6 follows from geometric ergodicity of $X$ ; independence and time-stationary of $Y$ ; and integrability (over the stationary distribution of $X$ ) of $d(\\vartheta_{0},X_{n},i)$ ; for each $i.$ : Given that part (i) holds, part (ii) follows from assuming first-moment continuity (as in Hansen, 1982) of $d(\\vartheta_{n},X_{n},Y_{n})$ at $\\vartheta_{0}$ :\n\nTheorem C.2 (Asymptotic normality). Under Assumptions C.1\u2013C.6, $\\sqrt{N}(\\mathbb{\\vartheta}_{n}-\\mathbb{\\vartheta}_{0})$ converges in distribution as $N\\to\\infty$ to a normal random vector with mean zero and covariance matrix\n\n$$\n\\Lambda = \\left(d_0^\\top \\mathcal{W}_0 d_0\\right)^{-1} d_0^\\top \\mathcal{W}_0 \\Sigma_0 \\mathcal{W}_0 d_0 \\left(d_0^\\top \\mathcal{W}_0 d_0\\right)^{-1} . \\tag{C.8}\n$$\n\nThe proof is a standard application of the mean-value theorem (for example, Hamilton, 1994), and omitted. The asymptotic covariance matrix $\\varLambda$ differs from its GMM counterpart in that $d_{0}$ is affected by the dependence of $V^{\\vartheta}$ on W and $\\{\\tau_{n},k_{n}\\}$ :\n\nFor the usual two-step GMM of Hansen, 1982, under which the distance matrices are chosen so that $\\mathcal{W}_{0}=\\Sigma_{0}^{-1}$ ; we have $\\varLambda=(d_{0}^{\\top}\\Sigma_{0}^{-I}d_{0})^{-I}$ : Our setting is that of an exactly identified GMM estimator ${\\bf\\nabla}n_{h}=n_{\\vartheta}$ ; $d_{0}$ is of rank $n_{\\vartheta}$ ; and $\\mathcal{W}_{0}$ is the identify matrix), so $A=d_{0}^{\\top}\\thinspace\\ensuremath{\\boldsymbol{{Sigma}}}_{0}(d_{0}^{\\top})^{-1}$ :\n\n## Appendix D. A recursive formula for conditional moments\n\nTo facilitate our analyses of the constant component of the jump-arrival intensity, we assume the intensity to be $\\lambda_{0}+\\lambda_{1}V_{t}$ ; for some non-negative constants $\\lambda_{0}$ and $\\lambda_{1}$ : Under certain integrability conditions (Duffie et al., 2000), the date- $n$ conditional moment-generating function of $(y_{n+1},V_{n+1})$ can be derived as $\\phi(u_{y},u_{v},V_{n})$ ; with $\\phi$ defined by\n\n$$\n\\phi(u_y, u_v, v) = \\exp\\left(A(u_y, u_v) + B(u_y, u_v)v\\right), \\tag{D.1}\n$$\n\nwhere, letting $b=\\sigma_{v}\\rho u_{v}-\\kappa_{v}$ $\\dot{\\mathbf{\\rho}}_{v},\\mathbf{\\rho}_{a}=-u_{\\nu}^{2}-2u_{y}[\\eta^{s}-1/2-\\lambda_{1}\\mu^{*}]-2\\lambda_{1}(\\exp(u_{y}\\mu_{J}+$ $u_{y}^{2}\\sigma_{J}^{2}/2)-1)$ ; and $\\gamma=\\sqrt{b^{2}+a\\sigma_{v}^{2}}$ ; the coefficients $A$ and $B$ are defined by\n\n$$\nB(u_y, u_v) = -\\frac{a(1 - \\exp(-\\gamma\\Delta)) - u_v[2\\gamma - (\\gamma - b)(1 - \\exp(-\\gamma\\Delta))]}{2\\gamma - (\\gamma + b)(1 - \\exp(-\\gamma\\Delta)) - u_v\\sigma_v^2(1 - \\exp(-\\gamma\\Delta))},\n$$\n\nFrom the explicitly known moment-generating function $\\phi$ ; conditional moments of $(y,V)$ can be derived using Eq. (8). In this section, we provide the following\n\n<!-- Page:40 -->\n\nrecursive scheme that turns out to be very useful in practice,36\n\n$$\n\\begin{align} \n\\mathbf{E}_t\\left(y_{t+\\Delta}^{0} V_{t+\\Delta}^{m}\\right) &= \\sum_{j=0}^{m-1} C_{m-1}^{j} \\mathbf{E}_t\\left(y_{t+\\Delta}^{0} V_{t+\\Delta}^{j}\\right) p_{y,v}^{(0,m-j)}(V_t), \\quad m \\geqslant 1, \\\\ \n\\mathbf{E}_t\\left(y_{t+\\Delta}^{n} V_{t+\\Delta}^{m}\\right) &= \\sum_{i=0}^{n-1} \\sum_{j=0}^{m} C_{n-1}^{i} C_{m}^{j} \\mathbf{E}_t\\left(y_{t+\\Delta}^{i} V_{t+\\Delta}^{j}\\right) p_{y,v}^{(n-i,m-j)}(V_t), \\quad n \\geqslant 1, \\, m \\geqslant 0, \\tag{D.3} \n\\end{align}\n$$\n\nwhere, for any $n\\geqslant0$ and $0{\\leqslant}i{\\leqslant}n,C_{n}^{i}=n!/i!(n-i)!$ ; and where\n\n$$\np_{y,v}^{(i,j)}(V_t) = A_{y,v}^{(i,j)} + B_{y,v}^{(i,j)} V_t, \\tag{D.4}\n$$\n\nwhere $A_{y,v}^{(i,j)}$ and $B_{y,v}^{(i,j)}$ are constants that can be derived in a recursive fashion, as follows.\n\nWe first derive $B_{\\nu.v}^{(i,j)}$ for $i\\geqslant0$ and $j\\geqslant0$ : With \u2018\u2018initial\u2019\u2019 values of $B_{y,v}^{(0,1)}=$ $\\mathrm{exp}(-\\kappa\\varDelta)$ ; $B_{\\nu,v}^{(1,0)}=(\\eta^{\\ddot{s}}-{\\textstyle\\frac{1}{2}}+\\lambda_{1}(J_{1}-\\mu^{*}))f_{0}$ ; $B_{y,v}^{(2,0)}=(1+\\lambda_{1}J_{2})f_{0}-f_{1}B_{y,v}^{(1,0)}$ ; y; vand B ${\\cal B}_{y,v_{\\ast}}^{(1,1)}=\\sigma_{v}\\rho f_{0}^{-}+\\textstyle{\\frac{1}{2}}\\kappa f_{0}f_{1}+\\textstyle{\\frac{1}{2}}\\sigma_{v}^{2}f_{0}{\\cal B}_{y,v}^{(1,0)}$ recursively up to any order. We have ; the following formulas enable us to calculate\n\n$$\nB_{y,\\,v}^{(0,\\,m)} = \\frac{m}{2} f_0 \\sigma_v^2 B_{y,\\,v}^{(0,\\,m-1)}, \\quad m \\geqslant 2, \\quad B_{y,\\,v}^{(n,\\,0)} = \\lambda_1 J_n f_0 - \\frac{1}{2} \\sum_{i=1}^{n} C_n^i B_{y,\\,v}^{(i,\\,0)} f_{n-i}, \\quad n \\geqslant 3,\n$$\n\n$$\nB_{y,v}^{(n,1)} = \\frac{1}{2}\\kappa f_0 f_n + \\frac{1}{2}\\sigma_v^2 f_0 B_{y,v}^{(n,0)} - \\frac{1}{2}\\sum_{i=1}^{n-1} C_n^i f_i B_{y,v}^{(n-i,1)}, \\quad n \\geqslant 2,\n$$\n\n$$\nB_{y,v}^{(n,m)} = \\frac{m}{2}\\sigma_v^2 f_0 B_{y,v}^{(n,m-1)} - \\frac{1}{2}\\sum_{i=1}^{n} C_n^i f_i B_{y,v}^{(n-i,m)}, \\quad n \\geqslant 1, \\quad m \\geqslant 2,\n$$\n\nwhere, $J_{1}=\\mu_{J},J_{2}=\\sigma_{J}^{2}+\\mu_{J}^{2}$ ; and $J_{n}=J_{n-1}\\mu_{J}+(n-1)J_{n-2}\\sigma_{J}^{2}$ (for $n\\geqslant3.$ ) are the moments of the jump amplitude. The coefficients $f_{i}$ and $g_{i}$ are given by\n\n$$\ng_0 = 2, \\quad g_n = 2\\gamma_n + \\frac{1}{f_0} \\sum_{i=1}^{n-1} \\Gamma_{n-i} g_i, \\quad n \\geqslant 1,\n$$\n\nwhere\n\n$$\n\\Gamma_0 = \\frac{\\exp(-\\kappa\\Delta)}{\\kappa}, \\quad \\Gamma_n = -\\kappa\\Delta \\sum_{i=0}^{n-1} C_{n-1}^{i} \\gamma_{n-i} \\Gamma_i, \\quad n \\geqslant 1.\n$$\n\n---\n\n36For pure affne difusions, an alternative approach can be found in Liu (1997). Das and Sundaram (1999) provide central moments of $y$ ; up to the fourth order, for the special case of $\\lambda_{1}=0$ :\n\n<!-- Page:41 -->\n\nand\n\n$$\n\\gamma_{1} = -\\left(\\frac{\\sigma_{v}}{\\kappa}\\right)^{2}\\left(\\rho\\frac{\\kappa}{\\sigma_{v}} + \\eta^{s} - \\frac{1}{2} + \\lambda_{1}(J_{1} - \\mu^{*})\\right),\n$$",
"text_b": "Noxt $A_{\\nu,v}^{(i,j)}$ $i\\geqslant0$ $j\\geqslant0$ $A_{y,v}^{(0,1)}=\\kappa\\bar{v}f_{0}$ $A_{y,v}^{(0,1)}=(-\\lambda_{0}\\mu^{*}+\\lambda_{0}J_{1})\\varLambda-(\\kappa\\gamma_{1}+\\sigma_{v}\\rho)(\\varLambda-f_{0})\\kappa\\bar{v}/\\sigma_{v}^{2}$ enable us to calculate $A_{y,v}^{(i,j)}$ recursively up to any order. We have\n\n$$\nA_{y,v}^{(0,n)} = \\frac{n-1}{2} \\sigma_{v}^{2} f_{0} A_{y,v}^{(0,n-1)}, \\, n \\geqslant 2, \\quad A_{y,v}^{(n,0)} = \\lambda_{0} J_{n}^{0} \\Delta - \\frac{\\kappa \\bar{V}}{\\sigma_{v}^{2}} \\left( \\kappa \\gamma_{n} \\Delta + \\hat{f}_{n} - \\hat{g}_{n} \\right), \\, n \\geqslant 2,\n$$\n\n$$\nA_{y,v}^{(n,1)} = -\\frac{\\kappa\\bar{v}}{2}f_0f_n - \\frac{1}{2}\\sum_{i=1}^{n-1} C_n^i f_i A_{y,v}^{(n-i,1)}, \\quad n \\geqslant 1,\n$$\n\n$$\nA_{y,v}^{(n,m)} = -\\kappa \\bar{v} \\sigma_v^{2(m-1)} m! f_n\\left(\\frac{f_0}{2}\\right)^m - \\frac{1}{2} \\sum_{i=1}^{n-1} \\hat{C}_n^i(m) f_i A_{y,v}^{(n-i,m)}, \\quad n \\geqslant 1, \\quad m \\geqslant 2,\n$$\n\nwhere for $n\\geqslant1,\\hat{C}_{n}^{0}(m)=m,\\hat{C}_{n}^{n}(m)=1,$ ; and, for $0<i<n$ ; $C_{n}^{i}(m)=C_{n-1}^{i}(m)+$ $C_{n-1}^{i-1}(m)$ : (Notice that, $C_{n}^{i}=n!/i!(n-i)$ ! defined previously, is a special case of $C_{n}^{i}(m)$ ; with $m=1.$ :)\n\nThe coefficients $\\hat{f}$ and $\\hat{g}$ are defined by\n\n$$\n\\hat{f}_1 = f_1, \\quad \\hat{f}_n = f_n - \\frac{1}{2} \\sum_{i=1}^{n-1} C_{n-1}^{n-i} \\hat{f}_i f_{n-i}, \\qquad \\hat{g}_1 = g_1 \\quad \\hat{g}_n = g_n - \\frac{1}{2} \\sum_{i=1}^{n-1} C_{n-1}^{n-i} \\hat{g}_i g_{n-i}.\n$$\n\n## Appendix E. Appendix to Section 5\n\n### E.1. Tests of moment conditions\n\nOur tests of moment conditions follow from the tests of orthogonality conditions developed in Eichenbaum et al. (1988), and are also closely related to the Hansen (1982) test of over-identifying restrictions (Lemma 4.1 of Hansen, 1982). Let $\\mathrm{E}_{n}(\\varepsilon_{n+1})=0$ be the $m=7$ moment conditions under consideration, and let $\\hat{\\mathcal{G}}_{N}$ be the exactly identified IS-GMM estimators, obtained from the \u2018\u2018optimal\u2019\u2019 moment condition $\\mathrm{E}_{n}(\\mathcal{H}_{n+1})=0$ : To test $\\mathrm{E}_{n}(\\varepsilon_{n+1})=0$ we construct its sample analogue by\n\n$$\n\\mathcal{G}_N\\left(\\hat{\\theta}_N\\right) = \\frac{1}{N} \\sum_{n \\leqslant N} \\varepsilon_n\\left(\\hat{\\theta}_N\\right), \\tag{E.1}\n$$\n\n<!-- Page:42 -->\n\nwhere $\\varepsilon_{n}(\\hat{\\mathcal{G}}_{N})$ denotes evaluating the moments e at the IS-GMM estimator $\\hat{\\mathcal{A}}_{N}$ : Using arguments similar to those following Assumption C.5 in Section 3, one can show that, under typical technical regularity conditions, $\\sqrt{N}\\mathcal{G}_{N}(\\hat{\\mathcal{G}}_{0})$ is asymptotically normal. Applying a standard mean-value expansion,\n\n$$\n\\mathcal{G}_N(\\hat{\\theta}_N) = \\mathcal{G}_N(\\theta_0) + \\left. \\frac{\\partial \\mathcal{G}_N(\\theta)}{\\partial \\theta} \\right|_{\\bar{\\theta}_N} (\\hat{\\theta}_N - \\theta_0) \\tag{E.2}\n$$\n\nwhere $\\bar{\\vartheta}_{N}^{j}$ is can be shown between $\\boldsymbol{\\vartheta}_{0}^{j}$ and $\\vartheta_{N}^{j}$ ; for $j\\in\\left\\{1,...,n_{\\vartheta}\\right\\}$ : Moreover, for sufficiently large and with probability arbitrarily close to one, we can write\n\n$$\n\\hat{\\theta}_N - \\theta_0 = -\\left(\\left.\\frac{\\partial G_N(\\theta)}{\\partial \\theta}\\right|_{\\tilde{\\theta}_N}\\right)^{-1} G_N(\\theta_0), \\tag{E.3}\n$$\n\nwhere $G_{N}=(N)^{-1}\\Sigma_{n}\\mathcal{H}_{n}$ is the sample analogue of the \u2018\u2018optimal\u2019\u2019 moments. We know that $\\partial G_{N}\\left(\\bar{\\mathcal{\\vartheta}}_{N}\\right)/\\partial\\vartheta$ converges to a constant full-rank matrix $d_{0}$ in probability, under Assumption C.6, using the fact that $\\hat{\\mathcal{S}}_{N}$ is estimated under an exactly identified IS-GMM setting.\n\nSubstituting (E.3) into (E.2), we obtain\n\n$$\n\\sqrt{N}g_N(\\hat{\\theta}_N) \\stackrel{a}{\\approx} \\sqrt{N}\\left(g_N(\\theta_0) - \\left.\\frac{\\partial \\mathcal{G}_N(\\theta)}{\\partial \\theta}\\right|_{\\bar{\\theta}_N}\\left(\\left.\\frac{\\partial G_N(\\theta)}{\\partial \\theta}\\right|_{\\bar{\\theta}_N}\\right)^{-1}G_N(\\theta_0)\\right), \\tag{E.4}\n$$\n\nwhere $\\approx^{a}$ means \u2018\u2018asymptotically equivalent in distribution to.\u2019\u2019 Thus, $\\mathcal{G}_{N}(\\hat{\\mathcal{G}}_{N})$ is asymptotically normal with some covariance matrix $\\Omega$ : An estimator $\\Omega_{N}$ of $\\Omega$ can be obtained by estimating the covariance matrix of the right-hand side of (E.4).\n\nThe $m$ moment conditions can be tested either individually or jointly. We can test the ith moment condition by using the fact that $\\sqrt{N}\\mathcal{G}_{N}^{i}(\\hat{\\mathcal{Y}}_{N})/\\sqrt{(\\Omega_{N})_{i i}}$ is asymptotically standard normal. We can test any subgroup of moment conditions, indexed by $I$ ; by using the fact that, in large sample, $\\bar{N}(\\mathcal{G}_{N}^{I}(\\hat{\\mathcal{Y}}_{N}))^{\\top}/((\\Omega_{N})_{I})^{-1}\\mathcal{G}_{N}^{I}(\\hat{\\mathcal{Y}}_{N})$ is distributed as a $\\chi^{2}$ random variable with $\\#(I)$ degrees of freedom.\n\n### E.2. Estimation of interest rates and dividend yields\n\nFor the purpose of estimating the respective parameter vectors $\\theta_{r}$ and $\\theta_{q}$ of the short-rate process $r$ and the dividend-rate process $q$ defined by (3), we use, from Datastream, weekly time-series of 3-month LIBOR rates and S&P 500 composite dividend yields from January 1987 to December 1996.\n\nFixing a sampling interval , and taking advantage of the fact that the conditional density of $q_{n}$ given $q_{n-1}$ is that of a non-central $\\chi^{2}$ (Feller, 1951; Cox et al., 1985), we estimate $\\theta_{q}$ using MLE. The time series of S&P 500 composite dividend yields is used as a proxy for $\\{q_{n}\\}$ : The observed $T$ -year LIBOR rates $\\{R_{n}\\}$ (converted to continuous compounding rates) can be expressed in terms of $r_{n}$ by (Cox et al., 1985)\n\n$$\nR_n = - \\frac{1}{T} (\\alpha_r(0, T, \\theta_r^0) + \\beta_r(0, T, \\theta_r^0) r_n),\n$$\n\n<!-- Page:43 -->\n\n<center><i>Table 6 ML estimates of interest rates <img src=\"https://latex.codecogs.com/svg.image?r\" style=\"vertical-align: middle; height: 1.2em;\" alt=\"r\" class=\"latex-formula\"/> and dividend yields <img src=\"https://latex.codecogs.com/svg.image?q\" style=\"vertical-align: middle; height: 1.2em;\" alt=\"q\" class=\"latex-formula\"/> a </i></center>\n\n| $\\kappa_{r}$ | $\\bar{r}$ | $\\sigma_{r}$ | $\\kappa_{q}$ | $\\bar{q}$ | $\\sigma_{q}$ |\n|---|---|---|---|---|---|\n| 0.20 | 0.058 | 0.0415 | 0.24 | 0.025 | 0.0269 |\n| (0.15) | (0.016) | (0.0009) | (0.33) | (0.011) | (0.0004) |\n\n<center>\n<div style=\"display: inline-block; text-align: left; max-width: 80%;\">\n<i>aData: Weekly 3-month LIBOR rates and S&P 500 dividend yields, Jan. 1987-Dec. 1996. </i>\n</div>\n</center>\n\n<center><i>Table 7 A Monte Carlo studya </i></center>"
}

请严格按照以下JSON格式返回优化后的两个文本块:
{
"text_a": "优化后的第一个文本块",
"text_b": "优化后的第二个文本块"
}

重要说明:
- 保持所有原始内容的完全一致，不要添加或删除信息
- 只调整两个文本块之间的截断点位置
- 避免在表格、图表、公式、代码块等结构化内容中间截断
- 确保输出使用markdown内联latex格式，行内公式使用单个`$`符号包围，独立公式块使用双`$$`符号包围
- 保持所有数学符号的正确性和一致性
- 请只返回符合要求的JSON格式，不要包含任何其他解释或备注
请遵循以下优化原则:
1. 核心目标是让每个块在独立阅读时尽可能具有完整的语义和结构，例如如果可以识别，脚注内容和正文内容应放在同一个文本块中
2. 表格、图表、公式、代码块等结构化内容应作为整体放在同一个文本块中，不要在其中间截断
3. 优先在段落结束、章节标题前、主题转换处设置截断点
4. 不需要考虑两块文本长度的平衡，只关注语义完整性
5. 对于HTML/Markdown等标记语言，确保标签的完整性，不要截断标签结构


===== LLM RESPONSE =====

```json
{
"text_a": "where $g_{9}(c,\\vartheta,r,q,\\tau,k)=\\vartheta_{g}(c,\\vartheta,r,q,\\tau,k)/\\partial\\vartheta$ ; with $g$ defined by Eq. (C.1), and where $c_{i}=f(V_{i}^{A},\\vartheta_{0},r_{i}^{A},q_{i}^{A},\\tau_{i},k_{i})$ : The first term on the right-hand side of Eq. (C.7) arises from the explicit dependence of $h$ on W; while the second term arises from the dependence of $h$ on $V_{i}$ and the dependence of $V_{i}^{9}=g(c_{i},\\vartheta,r_{i}^{A},q_{i}^{A},\\tau_{i},k_{i})$ on W; for $i\\in\\{n-n_{v}+1,...,n\\}$ : This second term is important in identifying risk-premium parameters such as $\\eta^{v}$ : Intuitively, such parameters are identified by exploring the option-pricing relation through $V^{\\vartheta}$ :\n\nAssumption C.6 (Convergence of \u2018\u2018Jacobian estimator\u2019\u2019). For some constant $(n_{h}\\times n_{\\vartheta})$ matrix $d_{0}$ of rank $n_{\\vartheta}$ : (i) $1/N\\textstyle\\sum_{n\\leqslant N}d(\\vartheta_{0},X_{n},Y_{n})$ converges in probability as $N\\to\\infty$ to $d_{0}$ : (ii) For any $\\{\\mathcal{\\vartheta}_{n}\\}$ converging in probability as $n\\to\\infty$ to $\\vartheta_{0}$ ; $1/N\\textstyle\\sum_{n\\leqslant N}d(\\vartheta_{n},X_{n},Y_{n})$ converges in probability as $N\\to\\infty$ to $d_{0}$ :\n\n<!-- Page:39 -->\n\nPart (i) of Assumption C.6 follows from geometric ergodicity of $X$ ; independence and time-stationary of $Y$ ; and integrability (over the stationary distribution of $X$ ) of $d(\\vartheta_{0},X_{n},i)$ ; for each $i.$ : Given that part (i) holds, part (ii) follows from assuming first-moment continuity (as in Hansen, 1982) of $d(\\vartheta_{n},X_{n},Y_{n})$ at $\\vartheta_{0}$ :\n\nTheorem C.2 (Asymptotic normality). Under Assumptions C.1\u2013C.6, $\\sqrt{N}(\\mathbb{\\vartheta}_{n}-\\mathbb{\\vartheta}_{0})$ converges in distribution as $N\\to\\infty$ to a normal random vector with mean zero and covariance matrix\n\n$$\n\\Lambda = \\left(d_0^\\top \\mathcal{W}_0 d_0\\right)^{-1} d_0^\\top \\mathcal{W}_0 \\Sigma_0 \\mathcal{W}_0 d_0 \\left(d_0^\\top \\mathcal{W}_0 d_0\\right)^{-1} . \\tag{C.8}\n$$\n\nThe proof is a standard application of the mean-value theorem (for example, Hamilton, 1994), and omitted. The asymptotic covariance matrix $\\varLambda$ differs from its GMM counterpart in that $d_{0}$ is affected by the dependence of $V^{\\vartheta}$ on W and $\\{\\tau_{n},k_{n}\\}$ :\n\nFor the usual two-step GMM of Hansen, 1982, under which the distance matrices are chosen so that $\\mathcal{W}_{0}=\\Sigma_{0}^{-1}$ ; we have $\\varLambda=(d_{0}^{\\top}\\Sigma_{0}^{-I}d_{0})^{-I}$ : Our setting is that of an exactly identified GMM estimator ${\\bf\\nabla}n_{h}=n_{\\vartheta}$ ; $d_{0}$ is of rank $n_{\\vartheta}$ ; and $\\mathcal{W}_{0}$ is the identify matrix), so $A=d_{0}^{\\top}\\thinspace\\ensuremath{\\boldsymbol{{Sigma}}}_{0}(d_{0}^{\\top})^{-1}$ :\n\n## Appendix D. A recursive formula for conditional moments\n\nTo facilitate our analyses of the constant component of the jump-arrival intensity, we assume the intensity to be $\\lambda_{0}+\\lambda_{1}V_{t}$ ; for some non-negative constants $\\lambda_{0}$ and $\\lambda_{1}$ : Under certain integrability conditions (Duffie et al., 2000), the date- $n$ conditional moment-generating function of $(y_{n+1},V_{n+1})$ can be derived as $\\phi(u_{y},u_{v},V_{n})$ ; with $\\phi$ defined by\n\n$$\n\\phi(u_y, u_v, v) = \\exp\\left(A(u_y, u_v) + B(u_y, u_v)v\\right), \\tag{D.1}\n$$\n\nwhere, letting $b=\\sigma_{v}\\rho u_{v}-\\kappa_{v}$ $\\dot{\\mathbf{\\rho}}_{v},\\mathbf{\\rho}_{a}=-u_{\\nu}^{2}-2u_{y}[\\eta^{s}-1/2-\\lambda_{1}\\mu^{*}]-2\\lambda_{1}(\\exp(u_{y}\\mu_{J}+$ $u_{y}^{2}\\sigma_{J}^{2}/2)-1)$ ; and $\\gamma=\\sqrt{b^{2}+a\\sigma_{v}^{2}}$ ; the coefficients $A$ and $B$ are defined by\n\n$$\nB(u_y, u_v) = -\\frac{a(1 - \\exp(-\\gamma\\Delta)) - u_v[2\\gamma - (\\gamma - b)(1 - \\exp(-\\gamma\\Delta))]}{2\\gamma - (\\gamma + b)(1 - \\exp(-\\gamma\\Delta)) - u_v\\sigma_v^2(1 - \\exp(-\\gamma\\Delta))},\n$$\n\nFrom the explicitly known moment-generating function $\\phi$ ; conditional moments of $(y,V)$ can be derived using Eq. (8). In this section, we provide the following\n\n<!-- Page:40 -->\n\nrecursive scheme that turns out to be very useful in practice,36\n\n$$\n\\begin{align} \n\\mathbf{E}_t\\left(y_{t+\\Delta}^{0} V_{t+\\Delta}^{m}\\right) &= \\sum_{j=0}^{m-1} C_{m-1}^{j} \\mathbf{E}_t\\left(y_{t+\\Delta}^{0} V_{t+\\Delta}^{j}\\right) p_{y,v}^{(0,m-j)}(V_t), \\quad m \\geqslant 1, \\\\ \n\\mathbf{E}_t\\left(y_{t+\\Delta}^{n} V_{t+\\Delta}^{m}\\right) &= \\sum_{i=0}^{n-1} \\sum_{j=0}^{m} C_{n-1}^{i} C_{m}^{j} \\mathbf{E}_t\\left(y_{t+\\Delta}^{i} V_{t+\\Delta}^{j}\\right) p_{y,v}^{(n-i,m-j)}(V_t), \\quad n \\geqslant 1, \\, m \\geqslant 0, \\tag{D.3} \n\\end{align}\n$$\n\nwhere, for any $n\\geqslant0$ and $0{\\leqslant}i{\\leqslant}n,C_{n}^{i}=n!/i!(n-i)!$ ; and where\n\n$$\np_{y,v}^{(i,j)}(V_t) = A_{y,v}^{(i,j)} + B_{y,v}^{(i,j)} V_t, \\tag{D.4}\n$$\n\nwhere $A_{y,v}^{(i,j)}$ and $B_{y,v}^{(i,j)}$ are constants that can be derived in a recursive fashion, as follows.\n\nWe first derive $B_{\\nu.v}^{(i,j)}$ for $i\\geqslant0$ and $j\\geqslant0$ : With \u2018\u2018initial\u2019\u2019 values of $B_{y,v}^{(0,1)}=$ $\\mathrm{exp}(-\\kappa\\varDelta)$ ; $B_{\\nu,v}^{(1,0)}=(\\eta^{\\ddot{s}}-{\\textstyle\\frac{1}{2}}+\\lambda_{1}(J_{1}-\\mu^{*}))f_{0}$ ; $B_{y,v}^{(2,0)}=(1+\\lambda_{1}J_{2})f_{0}-f_{1}B_{y,v}^{(1,0)}$ ; y; vand B ${\\cal B}_{y,v_{\\ast}}^{(1,1)}=\\sigma_{v}\\rho f_{0}^{-}+\\textstyle{\\frac{1}{2}}\\kappa f_{0}f_{1}+\\textstyle{\\frac{1}{2}}\\sigma_{v}^{2}f_{0}{\\cal B}_{y,v}^{(1,0)}$ recursively up to any order. We have ; the following formulas enable us to calculate\n\n$$\nB_{y,\\,v}^{(0,\\,m)} = \\frac{m}{2} f_0 \\sigma_v^2 B_{y,\\,v}^{(0,\\,m-1)}, \\quad m \\geqslant 2, \\quad B_{y,\\,v}^{(n,\\,0)} = \\lambda_1 J_n f_0 - \\frac{1}{2} \\sum_{i=1}^{n} C_n^i B_{y,\\,v}^{(i,\\,0)} f_{n-i}, \\quad n \\geqslant 3,\n$$\n\n$$\nB_{y,v}^{(n,1)} = \\frac{1}{2}\\kappa f_0 f_n + \\frac{1}{2}\\sigma_v^2 f_0 B_{y,v}^{(n,0)} - \\frac{1}{2}\\sum_{i=1}^{n-1} C_n^i f_i B_{y,v}^{(n-i,1)}, \\quad n \\geqslant 2,\n$$\n\n$$\nB_{y,v}^{(n,m)} = \\frac{m}{2}\\sigma_v^2 f_0 B_{y,v}^{(n,m-1)} - \\frac{1}{2}\\sum_{i=1}^{n} C_n^i f_i B_{y,v}^{(n-i,m)}, \\quad n \\geqslant 1, \\quad m \\geqslant 2,\n$$\n\nwhere, $J_{1}=\\mu_{J},J_{2}=\\sigma_{J}^{2}+\\mu_{J}^{2}$ ; and $J_{n}=J_{n-1}\\mu_{J}+(n-1)J_{n-2}\\sigma_{J}^{2}$ (for $n\\geqslant3.$ ) are the moments of the jump amplitude. The coefficients $f_{i}$ and $g_{i}$ are given by\n\n$$\ng_0 = 2, \\quad g_n = 2\\gamma_n + \\frac{1}{f_0} \\sum_{i=1}^{n-1} \\Gamma_{n-i} g_i, \\quad n \\geqslant 1,\n$$\n\nwhere\n\n$$\n\\Gamma_0 = \\frac{\\exp(-\\kappa\\Delta)}{\\kappa}, \\quad \\Gamma_n = -\\kappa\\Delta \\sum_{i=0}^{n-1} C_{n-1}^{i} \\gamma_{n-i} \\Gamma_i, \\quad n \\geqslant 1.\n$$\n\n---\n\n36For pure affne difusions, an alternative approach can be found in Liu (1997). Das and Sundaram (1999) provide central moments of $y$ ; up to the fourth order, for the special case of $\\lambda_{1}=0$ :\n\n<!-- Page:41 -->\n\nand\n\n$$\n\\gamma_{1} = -\\left(\\frac{\\sigma_{v}}{\\kappa}\\right)^{2}\\left(\\rho\\frac{\\kappa}{\\sigma_{v}} + \\eta^{s} - \\frac{1}{2} + \\lambda_{1}(J_{1} - \\mu^{*})\\right),\n$$\nNoxt $A_{\\nu,v}^{(i,j)}$ $i\\geqslant0$ $j\\geqslant0$ $A_{y,v}^{(0,1)}=\\kappa\\bar{v}f_{0}$ $A_{y,v}^{(0,1)}=(-\\lambda_{0}\\mu^{*}+\\lambda_{0}J_{1})\\varLambda-(\\kappa\\gamma_{1}+\\sigma_{v}\\rho)(\\varLambda-f_{0})\\kappa\\bar{v}/\\sigma_{v}^{2}$ enable us to calculate $A_{y,v}^{(i,j)}$ recursively up to any order. We have\n\n$$\nA_{y,v}^{(0,n)} = \\frac{n-1}{2} \\sigma_{v}^{2} f_{0} A_{y,v}^{(0,n-1)}, \\, n \\geqslant 2, \\quad A_{y,v}^{(n,0)} = \\lambda_{0} J_{n}^{0} \\Delta - \\frac{\\kappa \\bar{V}}{\\sigma_{v}^{2}} \\left( \\kappa \\gamma_{n} \\Delta + \\hat{f}_{n} - \\hat{g}_{n} \\right), \\, n \\geqslant 2,\n$$\n\n$$\nA_{y,v}^{(n,1)} = -\\frac{\\kappa\\bar{v}}{2}f_0f_n - \\frac{1}{2}\\sum_{i=1}^{n-1} C_n^i f_i A_{y,v}^{(n-i,1)}, \\quad n \\geqslant 1,\n$$\n\n$$\nA_{y,v}^{(n,m)} = -\\kappa \\bar{v} \\sigma_v^{2(m-1)} m! f_n\\left(\\frac{f_0}{2}\\right)^m - \\frac{1}{2} \\sum_{i=1}^{n-1} \\hat{C}_n^i(m) f_i A_{y,v}^{(n-i,m)}, \\quad n \\geqslant 1, \\quad m \\geqslant 2,\n$$\n\nwhere for $n\\geqslant1,\\hat{C}_{n}^{0}(m)=m,\\hat{C}_{n}^{n}(m)=1,$ ; and, for $0<i<n$ ; $C_{n}^{i}(m)=C_{n-1}^{i}(m)+$ $C_{n-1}^{i-1}(m)$ : (Notice that, $C_{n}^{i}=n!/i!(n-i)$ ! defined previously, is a special case of $C_{n}^{i}(m)$ ; with $m=1.$ :)\n\nThe coefficients $\\hat{f}$ and $\\hat{g}$ are defined by\n\n$$\n\\hat{f}_1 = f_1, \\quad \\hat{f}_n = f_n - \\frac{1}{2} \\sum_{i=1}^{n-1} C_{n-1}^{n-i} \\hat{f}_i f_{n-i}, \\qquad \\hat{g}_1 = g_1 \\quad \\hat{g}_n = g_n - \\frac{1}{2} \\sum_{i=1}^{n-1} C_{n-1}^{n-i} \\hat{g}_i g_{n-i}.\n$$\n",
"text_b": "## Appendix E. Appendix to Section 5\n\n### E.1. Tests of moment conditions\n\nOur tests of moment conditions follow from the tests of orthogonality conditions developed in Eichenbaum et al. (1988), and are also closely related to the Hansen (1982) test of over-identifying restrictions (Lemma 4.1 of Hansen, 1982). Let $\\mathrm{E}_{n}(\\varepsilon_{n+1})=0$ be the $m=7$ moment conditions under consideration, and let $\\hat{\\mathcal{G}}_{N}$ be the exactly identified IS-GMM estimators, obtained from the \u2018\u2018optimal\u2019\u2019 moment condition $\\mathrm{E}_{n}(\\mathcal{H}_{n+1})=0$ : To test $\\mathrm{E}_{n}(\\varepsilon_{n+1})=0$ we construct its sample analogue by\n\n$$\n\\mathcal{G}_N\\left(\\hat{\\theta}_N\\right) = \\frac{1}{N} \\sum_{n \\leqslant N} \\varepsilon_n\\left(\\hat{\\theta}_N\\right), \\tag{E.1}\n$$\n\n<!-- Page:42 -->\n\nwhere $\\varepsilon_{n}(\\hat{\\mathcal{G}}_{N})$ denotes evaluating the moments e at the IS-GMM estimator $\\hat{\\mathcal{A}}_{N}$ : Using arguments similar to those following Assumption C.5 in Section 3, one can show that, under typical technical regularity conditions, $\\sqrt{N}\\mathcal{G}_{N}(\\hat{\\mathcal{G}}_{0})$ is asymptotically normal. Applying a standard mean-value expansion,\n\n$$\n\\mathcal{G}_N(\\hat{\\theta}_N) = \\mathcal{G}_N(\\theta_0) + \\left. \\frac{\\partial \\mathcal{G}_N(\\theta)}{\\partial \\theta} \\right|_{\\bar{\\theta}_N} (\\hat{\\theta}_N - \\theta_0) \\tag{E.2}\n$$\n\nwhere $\\bar{\\vartheta}_{N}^{j}$ is can be shown between $\\boldsymbol{\\vartheta}_{0}^{j}$ and $\\vartheta_{N}^{j}$ ; for $j\\in\\left\\{1,...,n_{\\vartheta}\\right\\}$ : Moreover, for sufficiently large and with probability arbitrarily close to one, we can write\n\n$$\n\\hat{\\theta}_N - \\theta_0 = -\\left(\\left.\\frac{\\partial G_N(\\theta)}{\\partial \\theta}\\right|_{\\tilde{\\theta}_N}\\right)^{-1} G_N(\\theta_0), \\tag{E.3}\n$$\n\nwhere $G_{N}=(N)^{-1}\\Sigma_{n}\\mathcal{H}_{n}$ is the sample analogue of the \u2018\u2018optimal\u2019\u2019 moments. We know that $\\partial G_{N}\\left(\\bar{\\mathcal{\\vartheta}}_{N}\\right)/\\partial\\vartheta$ converges to a constant full-rank matrix $d_{0}$ in probability, under Assumption C.6, using the fact that $\\hat{\\mathcal{S}}_{N}$ is estimated under an exactly identified IS-GMM setting.\n\nSubstituting (E.3) into (E.2), we obtain\n\n$$\n\\sqrt{N}g_N(\\hat{\\theta}_N) \\stackrel{a}{\\approx} \\sqrt{N}\\left(g_N(\\theta_0) - \\left.\\frac{\\partial \\mathcal{G}_N(\\theta)}{\\partial \\theta}\\right|_{\\bar{\\theta}_N}\\left(\\left.\\frac{\\partial G_N(\\theta)}{\\partial \\theta}\\right|_{\\bar{\\theta}_N}\\right)^{-1}G_N(\\theta_0)\\right), \\tag{E.4}\n$$\n\nwhere $\\approx^{a}$ means \u2018\u2018asymptotically equivalent in distribution to.\u2019\u2019 Thus, $\\mathcal{G}_{N}(\\hat{\\mathcal{G}}_{N})$ is asymptotically normal with some covariance matrix $\\Omega$ : An estimator $\\Omega_{N}$ of $\\Omega$ can be obtained by estimating the covariance matrix of the right-hand side of (E.4).\n\nThe $m$ moment conditions can be tested either individually or jointly. We can test the ith moment condition by using the fact that $\\sqrt{N}\\mathcal{G}_{N}^{i}(\\hat{\\mathcal{Y}}_{N})/\\sqrt{(\\Omega_{N})_{i i}}$ is asymptotically standard normal. We can test any subgroup of moment conditions, indexed by $I$ ; by using the fact that, in large sample, $\\bar{N}(\\mathcal{G}_{N}^{I}(\\hat{\\mathcal{Y}}_{N}))^{\\top}/((\\Omega_{N})_{I})^{-1}\\mathcal{G}_{N}^{I}(\\hat{\\mathcal{Y}}_{N})$ is distributed as a $\\chi^{2}$ random variable with $\\#(I)$ degrees of freedom.\n\n### E.2. Estimation of interest rates and dividend yields\n\nFor the purpose of estimating the respective parameter vectors $\\theta_{r}$ and $\\theta_{q}$ of the short-rate process $r$ and the dividend-rate process $q$ defined by (3), we use, from Datastream, weekly time-series of 3-month LIBOR rates and S&P 500 composite dividend yields from January 1987 to December 1996.\n\nFixing a sampling interval , and taking advantage of the fact that the conditional density of $q_{n}$ given $q_{n-1}$ is that of a non-central $\\chi^{2}$ (Feller, 1951; Cox et al., 1985), we estimate $\\theta_{q}$ using MLE. The time series of S&P 500 composite dividend yields is used as a proxy for $\\{q_{n}\\}$ : The observed $T$ -year LIBOR rates $\\{R_{n}\\}$ (converted to continuous compounding rates) can be expressed in terms of $r_{n}$ by (Cox et al., 1985)\n\n$$\nR_n = - \\frac{1}{T} (\\alpha_r(0, T, \\theta_r^0) + \\beta_r(0, T, \\theta_r^0) r_n),\n$$\n\n<!-- Page:43 -->\n\n<center><i>Table 6 ML estimates of interest rates <img src=\"https://latex.codecogs.com/svg.image?r\" style=\"vertical-align: middle; height: 1.2em;\" alt=\"r\" class=\"latex-formula\"/> and dividend yields <img src=\"https://latex.codecogs.com/svg.image?q\" style=\"vertical-align: middle; height: 1.2em;\" alt=\"q\" class=\"latex-formula\"/> a </i></center>\n\n| $\\kappa_{r}$ | $\\bar{r}$ | $\\sigma_{r}$ | $\\kappa_{q}$ | $\\bar{q}$ | $\\sigma_{q}$ |\n|---|---|---|---|---|---|\n| 0.20 | 0.058 | 0.0415 | 0.24 | 0.025 | 0.0269 |\n| (0.15) | (0.016) | (0.0009) | (0.33) | (0.011) | (0.0004) |\n\n<center>\n<div style=\"display: inline-block; text-align: left; max-width: 80%;\">\n<i>aData: Weekly 3-month LIBOR rates and S&P 500 dividend yields, Jan. 1987-Dec. 1996. </i>\n</div>\n</center>\n\n<center><i>Table 7 A Monte Carlo studya </i></center>"
}
```

