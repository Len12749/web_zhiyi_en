#!/usr/bin/env python3
"""
LiteLLM Proxy API æµ‹è¯•éªŒè¯è„šæœ¬
ç®€æ´çš„åŠŸèƒ½éªŒè¯å·¥å…·
"""

import requests
import json
import time
from typing import Dict, Any

# é…ç½®
BASE_URL = "http://localhost:4000/v1"
API_KEY = "sk-litellm-master-key-2024"
HEADERS = {
    "Content-Type": "application/json",
    "Authorization": f"Bearer {API_KEY}"
}

def test_model(model_name: str, prompt: str = "Hello! Please respond briefly.") -> Dict[str, Any]:
    """æµ‹è¯•å•ä¸ªæ¨¡å‹"""
    print(f"\nğŸ” æµ‹è¯•æ¨¡å‹: {model_name}")
    
    data = {
        "model": model_name,
        "messages": [{"role": "user", "content": prompt}],
        "temperature": 0.7
    }
    
    start_time = time.time()
    
    try:
        response = requests.post(f"{BASE_URL}/chat/completions", headers=HEADERS, json=data, timeout=30)
        elapsed = time.time() - start_time
        
        if response.status_code == 200:
            result = response.json()
            content = result['choices'][0]['message'].get('content', '')
            tokens = result.get('usage', {}).get('total_tokens', 0)
            
            print(f"  âœ… æˆåŠŸ - {elapsed:.2f}s - {tokens} tokens")
            print(f"  ğŸ“ å“åº”: {content[:100]}...")
            
            return {
                "status": "success",
                "response_time": elapsed,
                "tokens": tokens,
                "content": content
            }
        else:
            print(f"  âŒ HTTP {response.status_code}: {response.text[:100]}")
            return {"status": "failed", "error": f"HTTP {response.status_code}"}
            
    except Exception as e:
        print(f"  âŒ å¼‚å¸¸: {e}")
        return {"status": "failed", "error": str(e)}

def test_system_status() -> Dict[str, Any]:
    """æµ‹è¯•ç³»ç»ŸçŠ¶æ€"""
    print("ğŸ“Š æ£€æŸ¥ç³»ç»ŸçŠ¶æ€...")
    
    try:
        # è·å–æ¨¡å‹åˆ—è¡¨
        response = requests.get(f"{BASE_URL}/models", headers={"Authorization": f"Bearer {API_KEY}"}, timeout=10)
        
        if response.status_code == 200:
            models_data = response.json()
            models = models_data.get("data", [])
            
            print(f"  âœ… ç³»ç»Ÿè¿è¡Œæ­£å¸¸")
            print(f"  ğŸ“‹ å¯ç”¨æ¨¡å‹: {len(models)} ä¸ª")
            
            return {"status": "healthy", "model_count": len(models)}
        else:
            print(f"  âŒ ç³»ç»Ÿå¼‚å¸¸: HTTP {response.status_code}")
            return {"status": "unhealthy", "error": f"HTTP {response.status_code}"}
            
    except Exception as e:
        print(f"  âŒ ç³»ç»Ÿå¼‚å¸¸: {e}")
        return {"status": "unhealthy", "error": str(e)}

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("=" * 60)
    print("ğŸš€ LiteLLM Proxy API æµ‹è¯•éªŒè¯")
    print("=" * 60)
    
    # 1. ç³»ç»ŸçŠ¶æ€æ£€æŸ¥
    system_status = test_system_status()
    if system_status["status"] != "healthy":
        print("\nâŒ ç³»ç»ŸçŠ¶æ€å¼‚å¸¸ï¼Œåœæ­¢æµ‹è¯•")
        return
    
    # 2. æµ‹è¯•ä¸»è¦æ¨¡å‹
    test_models = [
        "deepseek-ai/DeepSeek-V3",      # å¿«é€Ÿæ–‡æœ¬æ¨¡å‹
        "Qwen/QwQ-32B",                 # æ¨ç†æ¨¡å‹
        "gemini-2.5-flash",             # Geminiå¿«é€Ÿæ¨¡å‹
        "Qwen/Qwen2.5-VL-32B-Instruct"  # è§†è§‰æ¨¡å‹
    ]
    
    results = {}
    for model in test_models:
        results[model] = test_model(model)
    
    # 3. ç»Ÿè®¡ç»“æœ
    print("\n" + "=" * 60)
    print("ğŸ“‹ æµ‹è¯•ç»“æœæ€»ç»“")
    print("=" * 60)
    
    successful_models = [m for m, r in results.items() if r["status"] == "success"]
    failed_models = [m for m, r in results.items() if r["status"] == "failed"]
    
    print(f"âœ… æˆåŠŸæ¨¡å‹: {len(successful_models)}/{len(test_models)}")
    for model in successful_models:
        result = results[model]
        print(f"   - {model}: {result['response_time']:.2f}s, {result['tokens']} tokens")
    
    if failed_models:
        print(f"\nâŒ å¤±è´¥æ¨¡å‹: {len(failed_models)}")
        for model in failed_models:
            print(f"   - {model}: {results[model]['error']}")
    
    # 4. æ€»ä½“è¯„ä¼°
    success_rate = len(successful_models) / len(test_models) * 100
    avg_response_time = sum(r["response_time"] for r in results.values() if r["status"] == "success") / len(successful_models) if successful_models else 0
    
    print(f"\nğŸ¯ ç³»ç»Ÿè¯„ä¼°:")
    print(f"   æ¨¡å‹å¯ç”¨ç‡: {success_rate:.1f}%")
    print(f"   å¹³å‡å“åº”æ—¶é—´: {avg_response_time:.2f}s")
    
    if success_rate >= 75:
        print(f"   ğŸ‰ ç³»ç»Ÿè¿è¡ŒçŠ¶æ€: è‰¯å¥½")
    elif success_rate >= 50:
        print(f"   âš ï¸ ç³»ç»Ÿè¿è¡ŒçŠ¶æ€: ä¸€èˆ¬")
    else:
        print(f"   âŒ ç³»ç»Ÿè¿è¡ŒçŠ¶æ€: éœ€è¦æ£€æŸ¥")
    
    print(f"\nğŸ“– è¯¦ç»†ä½¿ç”¨è¯´æ˜è¯·å‚è€ƒ: API_USAGE_GUIDE.md")

if __name__ == "__main__":
    main() 